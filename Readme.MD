# Hi there! ğŸ‘‹ I'm Vinod Anbalagan

<!-- Animated typing with clean background -->
<div align="center">
    <img width="100%" height="300" src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=004d40,00bcd4,4caf50&height=300&section=header&text=Vinod%20Anbalagan&fontSize=50&fontColor=fff&animation=twinkling&fontAlignY=35&desc=Operations%20â†’%20Analytics%20â†’%20AI/ML%20%7C%20Building%20Intelligent%20Systems&descAlignY=55&descSize=20"/>
    <img src="https://readme-typing-svg.herokuapp.com?font=JetBrains+Mono&size=28&duration=1500&pause=1500&color=00796B&center=true&vCenter=true&width=800&lines=Operations+Background+%7C+ML+Engineer;Efficiency-First+AI+Systems;Multimodal+Learning+%7C+Agent+Orchestration;Exploring+Embodied+Intelligence" alt="Typing SVG" />
</div>

## ğŸš€ About Me

**From Operations to AI/ML Engineering** â€“ I bring a unique perspective to machine learning: I understand **efficiency** (from 6 years in supply chain operations), **business impact** (from analytics work), and **production deployment** (from recent deep ML engineering).

**ğŸ¯ What Makes Me Different:** Most ML engineers optimize for accuracy. I optimize for **deployability, efficiency, and real-world constraints** because I've managed physical operations where compute, energy, and time actually matter.

**ğŸ“ˆ Career Evolution:** 
- **2017-2023**: Supply Chain Operations â†’ Business Analytics â†’ Data Science (Toronto)
- **2024**: Deep dive into production ML systems (M2M Tech multimodal AI project)
- **2024-Now**: Building AI agents, researching model architectures

**ğŸ”¬ Current Research Interests:** 
- **Embodied AI & World Models**: Moving beyond text-only transformers toward physically-grounded intelligence
- **Efficient Architectures**: State Space Models (Mamba), JEPA, alternatives to O(nÂ²) attention
- **Neuro-Symbolic AI**: Combining neural networks with explicit graph structure (GNNs) for compositional reasoning
- **Multi-Agent Systems**: Intelligent orchestration and emergent behavior

**ğŸ› ï¸ Recent Projects:** 
- Multi-agent research assistant (Google Gemini orchestration, 40-60% faster paper comprehension)
- Model merging investigation (FrankenBERT - exposed catastrophic interference in naive merging)
- Multilingual video pipeline (Whisper + mBART + synthesis, 50+ languages, 95% accuracy)

**âœï¸ Technical Writing:** Publishing deep-dives on AI fundamentals, architecture evolution, and the path to AGI at [**The Meta Gradient**](https://substack.com/@vinodanbalagan)

**ğŸ“ Formal Training:** 
- University of Toronto (ML & Data Science Certification, 2024)
- Stanford University (ML Specialization, 2024)
- University of Windsor (M.A.Sc. Electrical Engineering, 2017)
- Anna University (B.E. Electronics & Communication, 2012)

**ğŸŒ What I'm Looking For:** 
- **Master's programs** in AI/ML (Europe, Fall 2026) focusing on embodied AI, efficient architectures, multimodal learning
- Research collaborations on state space models, neuro-symbolic AI, world models
- Open-source contributors for experimental projects exploring alternatives to transformer scaling

---

## ğŸ”¬ Currently Working On

- ğŸ§  **State Space Models (Mamba)**: Investigating linear-time alternatives to quadratic transformers for sequential modeling
- ğŸ¤– **Multi-Agent Orchestration**: Building intelligent routing between specialized AI agents (Google Gemini)
- ğŸ¯ **Neuro-Symbolic Architectures**: Exploring GNN + SSM hybrids for physical reasoning tasks
- ğŸ“š **Research Tools**: AI agents for accelerating paper comprehension and knowledge retention
- âœï¸ **Technical Writing**: Documenting the journey from RNNs â†’ Transformers â†’ State Space Models

---

## ğŸ† Featured Projects

### ğŸ“š SSM Learning Journey | In Progress
**Deep Dive into Sequence Models | Dec 2024 - Feb 2025**

**The Goal:** Understand sequence modeling evolution by implementing 
each architecture from scratch.

**Current Progress:**
- âœ… Week 1-2: RNN/LSTM vanishing gradient visualization
- ğŸ”„ Week 3-4: Transformer complexity analysis (in progress)
- ğŸ“… Week 5-8: Mamba implementation (planned)

**Why This Matters:** Moving beyond just using libraries to 
understanding *why* architectures evolved. Documenting insights 
on [The Meta Gradient](substack.com/@vinodanbalagan).

**Tech Stack:** PyTorch (from scratch), comparative benchmarking, 
physics-based tasks

---

### ğŸ¤– Research Paper Chat | Multi-Agent AI System
**Google 5-Day AI Intensive Capstone | Nov 2024**

[ğŸš€ Live Demo](https://vinodanbalagan-research-paper-chat-app-x3nxo4.streamlit.app/) | [ğŸ“ Technical Writeup](https://substack.com/@vinodanbalagan/note/p-180365459?utm_source=notes-share-action&r=g5tza)

**The Problem:** Reading ML papers takes 2-4 hours. Math notation is dense. Code snippets require translation. Concepts need context.

**The Solution:** Domain-specialized AI agents (Math, Code, Concept, Quiz, Chat) with intelligent orchestration.

**Why It's Interesting:**
- **Agent Routing**: Analyzes query intent, routes to appropriate specialist (not just keyword matching)
- **Context Management**: Maintains conversation state across agent switches
- **Production-Ready**: Error handling, cost optimization (~$0.001/query), deployed on Streamlit Cloud
- **Measurable Impact**: 40-60% faster paper comprehension in user testing

**Tech Stack:** Google Gemini 2.0 Flash, Streamlit, PDF parsing (regex-based section extraction), state management

**Key Insight:** Intelligence emerges from **coordination**, not just scale. Five small specialists outperform one large generalist for structured tasks.

---

ğŸ¬ Global Sound Evolution | Multilingual Video Pipeline
Production ML System | 2024
ğŸš€ Live Demo | ğŸ’» GitHub
What It Does: Transcribe â†’ Translate â†’ Re-synthesize video in 50+ languages
Technical Pipeline:

Speech-to-Text: OpenAI Whisper (noise-resilient, 95% accuracy)
Translation: mBART-50 (multilingual seq2seq)
Text-to-Speech: ElevenLabs API (natural voice cloning)
Video Synthesis: MoviePy (subtitle overlay + audio sync)

Production Lessons:

Long-form audio requires chunking strategy (avoid memory overflow)
Subtitle timing needs offset calibration (speech synthesis != original timing)
User feedback: "I can finally share my research talks with my non-English-speaking family"

**Note:** This was a pipeline integration project to understand 
multimodal AI workflows. The individual models (Whisper, mBART) 
are pre-trained; my contribution was the orchestration, error 
handling, and production deployment.


## ğŸ“ˆ Real-World Impact

### ğŸ¯ From My Analytics & Operations Work (2017-2023)

| Category | Project | Impact |
|----------|---------|--------|
| ğŸ’° Revenue Growth | Predictive Analytics (Organics Retailer) | **+25%** revenue |
| ğŸ’µ Cost Savings | Inventory Optimization (Whole Foods) | **$500K** prevented stockouts |
| âš™ï¸ Efficiency | Waste Reduction (Supply Chain) | **-20%** waste |
| â±ï¸ Automation | Reporting Systems | **15 hrs/week** saved |

**Key Takeaway:** I learned to build systems that **actually get used** in production, with measurable ROI. This operations mindset carries into my ML work.

---

## ğŸ“ Education & Training

### ğŸ« Formal Degrees
- ğŸ“ **University of Windsor** (2015-2017) - M.A.Sc. Electrical Engineering (Signal Processing, Control Systems)
- ğŸ“ **Anna University** (2008-2012) - B.E. Electronics & Communication Engineering

### ğŸ“š ML/AI Certifications (2024)
- **University of Toronto** - Machine Learning & Data Science Professional Certificate
- **Stanford University** - Machine Learning Specialization (Andrew Ng)
- **Google** - Advanced Data Analytics Professional Certificate
- **Wolfram Research** - ML Statistical Foundations
- **IBM** - AI Developer Professional Certificate
- **NVIDIA** - AI Operations & Infrastructure Fundamentals

### ğŸ¯ Currently Pursuing (2025-2026)
- **Master's in AI** - Applying to European programs (Netherlands, Belgium, Norway, Germany)
- **Research Focus**: State space models, neuro-symbolic AI, embodied intelligence, multimodal learning

---

## ğŸ›  Tech Stack

### Core ML/AI
<p align="left">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch" />
  <img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white" alt="TensorFlow" />
  <img src="https://img.shields.io/badge/Hugging%20Face-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black" alt="Hugging Face" />
  <img src="https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white" alt="Scikit-learn" />
  <img src="https://img.shields.io/badge/OpenCV-27338e?style=for-the-badge&logo=OpenCV&logoColor=white" alt="OpenCV" />
  <img src="https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white" alt="OpenAI" />
</p>

### Data & Analytics
<p align="left">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python" />
  <img src="https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white" alt="Pandas" />
  <img src="https://img.shields.io/badge/numpy-013243?style=for-the-badge&logo=numpy&logoColor=white" alt="NumPy" />
  <img src="https://img.shields.io/badge/SQL-4479A1?style=for-the-badge&logo=mysql&logoColor=white" alt="SQL" />
  <img src="https://img.shields.io/badge/Tableau-E97627?style=for-the-badge&logo=tableau&logoColor=white" alt="Tableau" />
</p>

### Deployment & MLOps
<p align="left">
  <img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Streamlit" />
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI" />
  <img src="https://img.shields.io/badge/Docker-2CA5E0?style=for-the-badge&logo=docker&logoColor=white" alt="Docker" />
  <img src="https://img.shields.io/badge/AWS-FF9900?style=for-the-badge&logo=amazonaws&logoColor=white" alt="AWS" />
  <img src="https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white" alt="Git" />
</p>

---

## ğŸ”¬ Research Philosophy

> **"Intelligence â‰  Language. True AI requires physical grounding, energy constraints, and embodied learning."**

### What I Believe:

**1. Beyond Text-Only LLMs**
- Current LLMs mimic the **output** of intelligence (language statistics)
- True intelligence requires understanding **causality** and **physics**
- A dog knows more about the world than GPT-4 (despite smaller "parameters")

**2. Efficiency as First Principle**
- Biological intelligence evolved to **optimize energy** in scarce environments
- AI without resource constraints has no incentive for efficiency, only verbosity
- My operations background: I've lived scarcity. I build systems that respect compute budgets.

**3. Architectural Innovation > Scaling**
- Transformers: O(nÂ²) attention is the bottleneck
- State Space Models (Mamba): O(n) recurrence with selective memory
- JEPA: Predict abstract states, not pixels (avoid "heavy library" of generative models)
- Neuro-symbolic: Graph structure + neural learning = compositional reasoning

**4. Embodied AI**
- Learning requires "jeopardy" (consequences, energy loss, failure)
- Multimodal fusion: Vision + touch + proprioception, not just text
- World models: Simulate physics internally before acting

### Applying to Master's Programs:

I'm seeking **research-oriented Master's programs** where I can explore:
- State space models for efficient sequential processing
- Neuro-symbolic architectures (GNNs + SSMs) for physical reasoning
- Embodied agents that learn from interaction, not just text
- Multimodal world models grounded in physics

**Target regions:** Netherlands, Belgium, Norway, Germany (Fall 2026)

---

## ğŸ“ Technical Writing

<div align="center">
  
  **âœï¸ [The Meta Gradient on Substack](https://substack.com/@vinodanbalagan)** 
  
  *Deep-dives into AI fundamentals, architecture evolution, and the path to AGI*
  
  **Upcoming Series:**
  - "The Evolutionary Journey: RNN â†’ LSTM â†’ Transformer â†’ Mamba"
  - "Why Transformers Can't Play Billiards: The Case for State Space Models"
  - "FrankenBERT: When AI Specialists Collide (Model Merging Research)"
  - "From LLMs to World Models: Yann LeCun's AMI Vision"

</div>

---

## ğŸ¯ What I'm Seeking

### Immediate (2024-2025):
- ğŸ”¬ **Research Collaborations** on state space models, neuro-symbolic AI, multi-agent systems
- ğŸ’¬ **Philosophical Discussions** on embodied intelligence, AGI, consciousness
- ğŸ§ª **Co-Experimenters** for projects exploring alternatives to transformer scaling
- ğŸ“ **Technical Writing** collaborations on AI architecture deep-dives

### Medium-Term (2026-2028):
- ğŸ“ **Master's in AI** (Europe, Fall 2026) - Applying to programs focused on:
  - Embodied AI & robotics (TU Delft, UvA OpenBots, NTNU)
  - Neuro-symbolic reasoning (Radboud, KU Leuven)
  - Efficient architectures (all programs)
- ğŸ”¬ **Thesis Research** on Mamba + JEPA + GNN world models for physical reasoning

### Long-Term Vision:
- Building AI systems that **understand causality**, not just pattern matching
- Contributing to the transition from language models â†’ world models
- Making AI **efficient enough** for embodied robots with real energy constraints

---

## ğŸ¤ Let's Connect!

<p align="center">
  <a href="https://www.linkedin.com/in/vinod-anbalagan/">
    <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn"/>
  </a>
  <a href="mailto:vinodanbalagan@gmail.com">
    <img src="https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white" alt="Email"/>
  </a>
  <a href="https://github.com/VinodAnbalagan">
    <img src="https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white" alt="GitHub"/>
  </a>
  <a href="https://huggingface.co/vinod-anbalagan">
    <img src="https://img.shields.io/badge/ğŸ¤—-Hugging%20Face-yellow?style=for-the-badge" alt="Hugging Face"/>
  </a>
  <a href="https://substack.com/@vinodanbalagan">
    <img src="https://img.shields.io/badge/Substack-FF6719?style=for-the-badge&logo=substack&logoColor=white" alt="Substack"/>
  </a>
</p>

<p align="center">
  <img src="https://komarev.com/ghpvc/?username=VinodAnbalagan&label=Profile%20views&color=0e75b6&style=flat" alt="profile views" />
</p>

---

<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&size=18&duration=3000&pause=1000&color=36BCF7&center=true&vCenter=true&width=900&lines=Operations+Background+â†’+ML+Engineering+â†’+Research;Building+Embodied+AI+%7C+State+Space+Models+%7C+Neuro-Symbolic+Systems;Efficiency-First+Architectures+Beyond+Transformer+Scaling;Applying+to+European+Master's+Programs+(Fall+2026);Open+to+Research+Collaborations+%26+Technical+Discussions" alt="Footer Typing SVG" />
</div>

---

<div align="center">
  <i>â­ï¸ From <a href="https://github.com/VinodAnbalagan">VinodAnbalagan</a></i><br>
  <i>"Intelligence emerges from constraints, not just compute. Building AI that understands physics, not just patterns."</i>
</div>
